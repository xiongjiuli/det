{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#  THE IPYNB FILE IS USED IN THE UII TO PRECESS DATA  #\n",
    "#                                                     #\n",
    "#######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.先把 mask 盖在图像上然后把图像拿出来看一看\n",
    "\n",
    "import torchio as tio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy.ndimage import binary_dilation\n",
    "import csv\n",
    "\n",
    "\n",
    "\n",
    "def dilate_mask(mask, iterations=20):\n",
    "    # 创建一个结构元素\n",
    "    # struct = np.ones((10, 10, 10))\n",
    "    # 使用binary_dilation函数进行膨胀\n",
    "    dilated_mask = binary_dilation(mask, iterations=iterations)\n",
    "    return dilated_mask\n",
    "\n",
    "\n",
    "def read_names_from_csv(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        names = []\n",
    "        for row in reader:\n",
    "            # print(row)\n",
    "            name = row[0]\n",
    "            names.append(name)\n",
    "    return names\n",
    "\n",
    "def extract_subvolume(arr):\n",
    "    # 找到非零元素的索引\n",
    "    nonzero_indices = np.transpose(np.nonzero(arr))\n",
    "    \n",
    "    if len(nonzero_indices) == 0:\n",
    "        return None, None  # 如果没有非零元素，则返回None\n",
    "    \n",
    "    # 获取包含所有非零元素的最小长方体的边界\n",
    "    min_indices = np.min(nonzero_indices, axis=0)\n",
    "    max_indices = np.max(nonzero_indices, axis=0) + 1\n",
    "    \n",
    "    # 提取最小长方体\n",
    "    subvolume = arr[min_indices[0]:max_indices[0], min_indices[1]:max_indices[1], min_indices[2]:max_indices[2]]\n",
    "    \n",
    "    return subvolume, (min_indices, max_indices)\n",
    "\n",
    "def extract_subarray(arr):\n",
    "    # 找到所有大于0的元素的坐标\n",
    "    coords = np.argwhere(arr > 0)\n",
    "    # 计算最小和最大的坐标\n",
    "    min_coords = coords.min(axis=0)\n",
    "    max_coords = coords.max(axis=0)\n",
    "    # 提取子数组\n",
    "    subarray = arr[min_coords[0]:max_coords[0]+1, min_coords[1]:max_coords[1]+1, min_coords[2]:max_coords[2]+1]\n",
    "    return subarray, min_coords, max_coords\n",
    "\n",
    "\n",
    "def transform_coords(coords, min_coords):\n",
    "    # 计算新坐标\n",
    "    new_coords = []\n",
    "    for c in coords:\n",
    "        c[0:3] = c[0:3] - min_coords\n",
    "        # c[3:6] = c[3:6] - min_coords\n",
    "        if min(c[0:3]) < 0:\n",
    "            print(min(c[0:3]))\n",
    "            print('ERROR!  the bbox in out of the mask aera!')\n",
    "        else:\n",
    "            new_coords.append(c)\n",
    "        \n",
    "    return new_coords\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def write_to_csv(filename, name, coords):\n",
    "    try:\n",
    "        # 打开文件以追加模式，如果文件不存在则创建\n",
    "        with open(filename, 'a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            \n",
    "            # 写入列表的内容\n",
    "            for coord in coords:\n",
    "                data_list = [name, coord[0], coord[1], coord[2], coord[3], coord[4], coord[5]]\n",
    "                writer.writerow(data_list)\n",
    "        \n",
    "        # print(f\"Data written to {filename} successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to {filename}: {str(e)}\")\n",
    "\n",
    "\n",
    "def process_nii_image(image_path, bboxes, output_path):\n",
    "    # 加载nii图像\n",
    "    img = tio.ScalarImage(image_path)\n",
    "    data = img.data[0, :, :, :]\n",
    "    # print(bboxes)\n",
    "    # 遍历每个框\n",
    "    for bbox in bboxes:\n",
    "        \n",
    "        x1, y1, z1, x2, y2, z2 = map(int, bbox)\n",
    "        # print(x1, y1, z1, x2, y2, z2)\n",
    "        # 将框内的像素值设置为50\n",
    "        # 将框的边线上的像素值设置为50\n",
    "        data[x1:x2+1, y1:y1+1, z1:z2+1] = 10\n",
    "        data[x1:x2+1, y2:y2+1, z1:z2+1] = 10\n",
    "        data[x1:x1+1, y1:y2+1, z1:z2+1] = 10\n",
    "        data[x2:x2+1, y1:y2+1, z1:z2+1] = 10\n",
    "        data[x1:x2+1, y1:y2+1, z1:z1+1] = 10\n",
    "        data[x1:x2+1, y1:y2+1, z2:z2+1] = 10\n",
    "        # print(f'data max is {data.max()}')\n",
    "    \n",
    "    # 保存结果\n",
    "    affine =  np.diag([-0.7, -0.7, 0.7, 1])\n",
    "    new_img = tio.ScalarImage(tensor=data.unsqueeze(0), affine=affine)\n",
    "    new_img.save(output_path)\n",
    "\n",
    "\n",
    "def convert_coordinates(coords): \n",
    "    x1y1z1 = []\n",
    "    for coord in coords:\n",
    "        # print(coord)\n",
    "        x, y, z, w, h, d = coord\n",
    "        x1 = x - w / 2. \n",
    "        y1 = y - h / 2. \n",
    "        z1 = z - d / 2. \n",
    "        x2 = x + w / 2. \n",
    "        y2 = y + h / 2. \n",
    "        z2 = z + d / 2. \n",
    "        x1y1z1.append([x1, y1, z1, x2, y2, z2])\n",
    "\n",
    "    return x1y1z1\n",
    "\n",
    "\n",
    "\n",
    "#* 先盖 mask\n",
    "parts = ['testing', 'training', 'validation']\n",
    "for part in parts:\n",
    "    csv_path = f'/data/julia/data_lymph/csv_file/part_{part}_names.csv'\n",
    "    names = read_names_from_csv(file_path=f'/data/julia/data_lymph/csv_file/{part}_names.csv')\n",
    "    csv_filename = f'/data/julia/data_lymph/anno/{part}_refine.csv'  # 文件名\n",
    "    # names = ['01190830220138']\n",
    "    test = 0\n",
    "    for name in tqdm(names):\n",
    "        if os.path.exists(f'/data/julia/data_lymph/{part}_processed/{name}.nii.gz'):\n",
    "            print(f\"File {name}.nii.gz exists. Skipping this iteration.\")\n",
    "            continue\n",
    "        # print(f\"Processing {file_name}...\")\n",
    "        img_path = f'/data/julia/data_lymph/{part}/{name}/'\n",
    "        file_name = os.listdir(img_path)\n",
    "        img = tio.ScalarImage(os.path.join(img_path, file_name[0]))\n",
    "\n",
    "        # * 窗宽窗位设置一下\n",
    "        clamped = tio.Clamp(out_min=-160., out_max=240.)\n",
    "        clamped_img = clamped(img)\n",
    "        # clamped_img.save(f'/data/julia/data_lymph/{part}_processed/{name}_clamp.nii.gz')\n",
    "\n",
    "        # * resample到（0.7， 0.7， 0.7）\n",
    "        resample = tio.Resample(0.7)\n",
    "        clamped_img = resample(clamped_img)\n",
    "        # print(clamped_img.spacing)\n",
    "        \n",
    "        # * 归一化到 0-1 之间\n",
    "        data_max = clamped_img.data.max()\n",
    "        data_min = clamped_img.data.min()\n",
    "        norm_data = (clamped_img.data - data_min) / (data_max - data_min)\n",
    "        # norm_img = tio.ScalarImage(tensor=norm_data, affine=clamped_img.affine)\n",
    "        # norm_img.save(f'/data/julia/data_lymph/{part}_processed/{name}_norm.nii.gz')\n",
    "\n",
    "        mask_path = f'/data/julia/data_lymph/{part}_mask/{name}/mediastinum.nii.gz'\n",
    "        mask = tio.LabelMap(mask_path)\n",
    "        mask = resample(mask)\n",
    "\n",
    "        # #* 盖上 mask\n",
    "        dilated_mask = dilate_mask(mask.data[0, :, :, :])\n",
    "        img_mask = norm_data * torch.tensor(dilated_mask).unsqueeze(0)\n",
    "        # img_mask = tio.ScalarImage(tensor=img_mask, affine=clamped_img.affine)\n",
    "        # img_mask.save(f'/data/julia/data_lymph/{part}_processed/{name}_dilatepro.nii.gz')\n",
    "\n",
    "        #! 到现在是一整张图像被处理结束， 然后下面开始处理想对应的坐标\n",
    "    \n",
    "        # * 这样的话先处理一下这个坐标吧\n",
    "\n",
    "        #* 读取csv文件中的世界坐标\n",
    "        # name = '01190830220138'\n",
    "        worldcoord = pd.read_csv(f'/data/julia/data_lymph/lymph_csv_refine/CTA_thin_std_{part}_lymph_refine.csv')\n",
    "        raw = worldcoord[worldcoord['image_path'].str.contains(name)]\n",
    "\n",
    "        coords = []\n",
    "        for i in range(len(raw)):\n",
    "            x = raw.iloc[i, 2]\n",
    "            y = raw.iloc[i, 3]\n",
    "            z = raw.iloc[i, 4]\n",
    "            width = raw.iloc[i, 5]\n",
    "            height = raw.iloc[i, 6]\n",
    "            depth = raw.iloc[i, 7]\n",
    "            coords.append([x, y, z, width, height, depth]) # 这个是世界坐标系\n",
    "        # print(coords)\n",
    "\n",
    "        # * 把世界坐标系转化为图像坐标系\n",
    "        # img = tio.ScalarImage('/data/julia/data_lymph/testing/01190830220138/std_1.0_6.nii.gz')\n",
    "        origin = img.origin\n",
    "        # print(f'the origin is {origin}')\n",
    "        img_coords = []\n",
    "        for coord in coords:\n",
    "            img_coord = (np.array(coord[0:3]) - np.array(origin) * np.array([-1., -1., 1.]) ) / np.array([0.7, 0.7, 0.7]) # img.spacing\n",
    "            coord[3: 6] = coord[3: 6] / np.array([0.7, 0.7, 0.7])\n",
    "            img_coords.append([img_coord[0], img_coord[1], img_coord[2], coord[3], coord[4], coord[5]])   #! xyzwhd\n",
    "\n",
    "\n",
    "        # 现在开始扣数据\n",
    "        #* 把数据分割出来，旁边的都是0的东西给去掉\n",
    "        # nii_img = tio.ScalarImage('/data/julia/data_lymph/testing_processed/01190830220138_dilatepro.nii.gz')\n",
    "        crop_img, coord = extract_subvolume(np.array(img_mask[0, :, :, :]))\n",
    "        crop_img = tio.ScalarImage(tensor=torch.tensor(crop_img).unsqueeze(0), affine=np.diag([-0.7, -0.7, 0.7, 1]))\n",
    "        crop_img.save(f'/data/julia/data_lymph/{part}_processed/{name}.nii.gz')\n",
    "        # print(coord)\n",
    "\n",
    "        new_coords = transform_coords(coords=img_coords, min_coords=coord[0])\n",
    "        # print(name)\n",
    "        # print(new_coords)\n",
    "\n",
    "        # 调用函数来写入数据\n",
    "        write_to_csv(csv_filename, name, new_coords)\n",
    "\n",
    "        if test == 0:\n",
    "            xyz_coords = convert_coordinates(img_coords)\n",
    "            process_nii_image(f'/data/julia/data_lymph/{part}_processed/{name}.nii.gz', xyz_coords, f'/data/julia/data_lymph/{part}_processed/{name}_testbbox.nii.gz')\n",
    "            test += 1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchio as tio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy.ndimage import binary_dilation\n",
    "import csv\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "\n",
    "\n",
    "def read_imgcoord_fromcsv(name, part):\n",
    "    #* 读取csv文件中的世界坐标\n",
    "    # name = '01190830220138'\n",
    "    imgcoord = pd.read_csv(f'/data/julia/data_lymph/anno/{part}_refine.csv')\n",
    "    raw = imgcoord[imgcoord['name']==\"'\" + name]\n",
    "    coords = []\n",
    "    for i in range(len(raw)):\n",
    "        x = raw.iloc[i, 1]\n",
    "        y = raw.iloc[i, 2]\n",
    "        z = raw.iloc[i, 3]\n",
    "        width = raw.iloc[i, 4]\n",
    "        height = raw.iloc[i, 5]\n",
    "        depth = raw.iloc[i, 6]\n",
    "        coords.append([x, y, z, width, height, depth]) # 这个是图像坐标系\n",
    "\n",
    "    return coords\n",
    "\n",
    "\n",
    "def create_hmap_v4(coordinates, shape):\n",
    "    arr = np.zeros(shape)\n",
    "    for coords in coordinates:\n",
    "        coord = [int(x) for x in coords[0:3]]\n",
    "        whd = [int(x) for x in coords[3:6]]\n",
    "        kernel = create_gaussian_kernel_v4(whd)\n",
    "        arr = place_gaussian(arr, kernel, coord)\n",
    "\n",
    "    return arr\n",
    "\n",
    "\n",
    "def create_gaussian_kernel_v4(whd):\n",
    "    size_max = int(np.max(whd))\n",
    "    size_min = int(np.min(whd))\n",
    "    size_mid = int(sorted(whd)[1])\n",
    "\n",
    "    array_large = create_gaussian_base(size_max, 0.5)\n",
    "    array_small = create_gaussian_base(size_min, 0.5)\n",
    "    array_midum = create_gaussian_base(size_mid, 0.5)\n",
    "\n",
    "    combined_kernel = combine_gaussian_kernels(array_large, array_small, array_midum)\n",
    "\n",
    "    return combined_kernel\n",
    "\n",
    "\n",
    "def create_gaussian_base(size, threshold):\n",
    "\n",
    "    if size <= 9:\n",
    "        _size = 9\n",
    "        half_dis = (_size + 1) / 2.\n",
    "    else:\n",
    "        _size = size\n",
    "        if _size % 2 != 1:  # 如果size是偶数就变成奇数\n",
    "            half_dis = _size / 2.\n",
    "            _size = _size + 1\n",
    "        else:\n",
    "            half_dis = (_size + 1) / 2.\n",
    "\n",
    "    if threshold == 0.5:\n",
    "        sigma = np.sqrt(half_dis**2 / (2 * np.log(2)))\n",
    "    elif threshold == 0.8:\n",
    "        sigma = np.sqrt(half_dis**2 / (2 * (np.log(5) - np.log(4))))\n",
    "    elif threshold == 0.3:\n",
    "        sigma = np.sqrt(half_dis**2 / (2 * (np.log(10) - np.log(3))))\n",
    "    else:\n",
    "        print(f'when x = distance, the y wrong input, now the threshold is {threshold}')\n",
    "\n",
    "    kernel = np.zeros((int(_size), int(_size), int(_size)))\n",
    "    center = tuple(s // 2 for s in (int(_size), int(_size), int(_size)))\n",
    "    kernel[center] = 1\n",
    "    gassian_kernel = gaussian_filter(kernel, sigma=sigma)\n",
    "\n",
    "    arr_min = gassian_kernel.min()\n",
    "    arr_max = gassian_kernel.max()\n",
    "    normalized_arr = (gassian_kernel - arr_min) / (arr_max - arr_min) # 归一化到 0-1 之间\n",
    "    # print(f'in the create_gaussian_base , the max is {normalized_arr.max()}, the min is {normalized_arr.min()}')\n",
    "    return normalized_arr\n",
    "\n",
    "\n",
    "def combine_gaussian_kernels(kernel_large, kernel_small, kernel_midum):\n",
    "    center_large = np.array(kernel_large.shape) // 2\n",
    "    small_shape = np.array(kernel_small.shape[0]) // 2\n",
    "    midum_shape = np.array(kernel_midum.shape[0]) // 2\n",
    "\n",
    "    kernel_large[center_large[0] - small_shape : center_large[0] + small_shape + 1, \n",
    "                 center_large[1] - small_shape : center_large[1] + small_shape + 1, \n",
    "                 center_large[2] - small_shape : center_large[2] + small_shape + 1, ] += kernel_small[:, :, :]\n",
    "    \n",
    "    kernel_large[center_large[0] - midum_shape : center_large[0] + midum_shape + 1, \n",
    "                 center_large[1] - midum_shape : center_large[1] + midum_shape + 1, \n",
    "                 center_large[2] - midum_shape : center_large[2] + midum_shape + 1, ] += kernel_midum[:, :, :]\n",
    "    \n",
    "    arr_min = kernel_large.min()\n",
    "    arr_max = kernel_large.max()\n",
    "    normalized_arr = (kernel_large - arr_min) / (arr_max - arr_min) # 归一化到 0-1 之间\n",
    "    # print(f'in the combine_gaussian_kernels , the max is {normalized_arr.max()}, the min is {normalized_arr.min()}')\n",
    "    return normalized_arr\n",
    "\n",
    "\n",
    "def place_gaussian(arr, kernel, pos):\n",
    "    x, y, z = pos\n",
    "    kx, ky, kz = kernel.shape\n",
    "    # 计算高斯核在数组中的位置\n",
    "    x1, x2 = max(0, x-kx//2), min(arr.shape[0], x+kx//2+1)\n",
    "    y1, y2 = max(0, y-ky//2), min(arr.shape[1], y+ky//2+1)\n",
    "    z1, z2 = max(0, z-kz//2), min(arr.shape[2], z+kz//2+1)\n",
    "    # 计算高斯核在自身中的位置\n",
    "    kx1, kx2 = max(0, kx//2-x), min(kx, kx//2-x+arr.shape[0])\n",
    "    ky1, ky2 = max(0, ky//2-y), min(ky, ky//2-y+arr.shape[1])\n",
    "    kz1, kz2 = max(0, kz//2-z), min(kz, kz//2-z+arr.shape[2])\n",
    "    # 将高斯核放置在指定位置\n",
    "    arr[x1:x2,y1:y2,z1:z2] = np.maximum(arr[x1:x2,y1:y2,z1:z2], kernel[kx1:kx2,ky1:ky2,kz1:kz2])\n",
    "\n",
    "    return arr\n",
    "\n",
    "\n",
    "nii_path = '/data/julia/data_lymph/testing_processed/202004290225_testbbox.nii.gz'\n",
    "name = '202004290225'\n",
    "imgcoords = read_imgcoord_fromcsv(name, part='testing')\n",
    "# print(imgcoords)\n",
    "nii_img = tio.ScalarImage(nii_path)\n",
    "shape = nii_img.shape\n",
    "print(shape)\n",
    "\n",
    "\n",
    "hmap = create_hmap_v4(imgcoords, shape[1:4])\n",
    "hmap_nii = tio.ScalarImage(tensor=torch.tensor(hmap).unsqueeze(0), affine=nii_img.affine)\n",
    "hmap_nii.save('/data/julia/data_lymph/hmap.nii')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
