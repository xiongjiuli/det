{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* 这个是为了生成nnunet的npz label的，要生成hmap， offset， whd， mask\n",
    "\n",
    "\n",
    "# 造高斯函数\n",
    "import numpy as np\n",
    "import torchio as tio\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.ndimage import zoom\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from scipy.ndimage import binary_dilation\n",
    "from pathlib import Path\n",
    "import random\n",
    "import shutil\n",
    "#* trans the data to the test\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def create_gaussian_base(size, threshold):\n",
    "\n",
    "    if size <= 9:\n",
    "        _size = 9\n",
    "        half_dis = (_size + 1) / 2.\n",
    "    else:\n",
    "        _size = size\n",
    "        if _size % 2 != 1:  # 如果size是偶数就变成奇数\n",
    "            half_dis = _size / 2.\n",
    "            _size = _size + 1\n",
    "        else:\n",
    "            half_dis = (_size + 1) / 2.\n",
    "\n",
    "    if threshold == 0.5:\n",
    "        sigma = np.sqrt(half_dis**2 / (2 * np.log(2)))\n",
    "    elif threshold == 0.8:\n",
    "        sigma = np.sqrt(half_dis**2 / (2 * (np.log(5) - np.log(4))))\n",
    "    elif threshold == 0.3:\n",
    "        sigma = np.sqrt(half_dis**2 / (2 * (np.log(10) - np.log(3))))\n",
    "    else:\n",
    "        print(f'when x = distance, the y wrong input, now the threshold is {threshold}')\n",
    "\n",
    "    kernel = np.zeros((int(_size), int(_size), int(_size)))\n",
    "    center = tuple(s // 2 for s in (int(_size), int(_size), int(_size)))\n",
    "    kernel[center] = 1\n",
    "    gassian_kernel = gaussian_filter(kernel, sigma=sigma)\n",
    "\n",
    "    arr_min = gassian_kernel.min()\n",
    "    arr_max = gassian_kernel.max()\n",
    "    normalized_arr = (gassian_kernel - arr_min) / (arr_max - arr_min) # 归一化到 0-1 之间\n",
    "    # print(f'in the create_gaussian_base , the max is {normalized_arr.max()}, the min is {normalized_arr.min()}')\n",
    "    return normalized_arr\n",
    "\n",
    "\n",
    "\n",
    "def create_gaussian_kernel_v5(whd):\n",
    "    # 定义新的维度\n",
    "    new_dims_w = int(whd[0])   # 新的长方体的维度\n",
    "    new_dims_h = int(whd[1])\n",
    "    new_dims_d = int(whd[2])\n",
    "    size_max = int(np.max(whd))\n",
    "\n",
    "\n",
    "    if new_dims_w % 2 == 0:\n",
    "        new_dims_w += 1\n",
    "    if new_dims_h % 2 == 0:\n",
    "        new_dims_h += 1\n",
    "    if new_dims_d % 2 == 0:\n",
    "        new_dims_d += 1\n",
    "    if size_max % 2 == 0:\n",
    "        size_max += 1\n",
    "\n",
    "    new_w = new_dims_w / size_max\n",
    "    new_h = new_dims_h / size_max\n",
    "    new_d = new_dims_d / size_max\n",
    "\n",
    "    gaussian_kernel = create_gaussian_base(size_max, 0.3)\n",
    "    # 使用scipy.ndimage.zoom函数来伸缩高斯核\n",
    "    # rescaled_kernel = zoom(gaussian_kernel, (new_dims_w, new_dims_h, new_dims_d))\n",
    "    rescaled_kernel = zoom(gaussian_kernel, (new_w, new_h, new_d))\n",
    "    rescaleded_kernel = add_dim_inarray(rescaled_kernel)\n",
    "    # print(f'rescaled_kernel.shape is {rescaled_kernel.shape}, and the rescaleded_kernel.shape is {rescaleded_kernel.shape}')\n",
    "\n",
    "    return rescaleded_kernel\n",
    "\n",
    "\n",
    "\n",
    "def add_dim_inarray(array):\n",
    "    shape = np.shape(array)\n",
    "    w, h, d = shape\n",
    "\n",
    "    if w % 2 == 0:\n",
    "        w += 1\n",
    "        new_array = np.ones((w, h, d))\n",
    "        # print((w-1)/2+ 1)\n",
    "        new_array[0:int((w-1)/2 + 1), :, :] = array[0:int((w-1)/2 + 1), :, :]\n",
    "        new_array[int((w-1)/2 + 1), :, :] = array[int((w-1)/2 ), :, :]\n",
    "        new_array[int((w-1)/2 + 2) : w+1 , :, :] = array[int((w-1)/2 + 1) : w, :, :]\n",
    "        array = new_array\n",
    "    if h % 2 == 0:\n",
    "        h += 1\n",
    "        new_array = np.ones((w, h, d))\n",
    "        new_array[:, 0:int((h-1)/2 + 1), :] = array[:, 0:int((h-1)/2 + 1), :]\n",
    "        new_array[:, int((h-1)/2 + 1), :] = array[:, int((h-1)/2 ), :]\n",
    "        new_array[:, int((h-1)/2 + 2) : h+1 , :] = array[:, int((h-1)/2 + 1) : h, :]\n",
    "        array = new_array\n",
    "    if d % 2 == 0:\n",
    "        d += 1\n",
    "        new_array = np.ones((w, h, d))\n",
    "        new_array[:, :, 0:int((d-1)/2 + 1)] = array[:, :, 0:int((d-1)/2 + 1)]\n",
    "        new_array[:, :, int((d-1)/2 + 1)] = array[:, :, int((d-1)/2 )]\n",
    "        new_array[:, :, int((d-1)/2 + 2) : d+1 ] = array[:, :, int((d-1)/2 + 1) : d]\n",
    "        array = new_array\n",
    "\n",
    "    return array\n",
    "\n",
    "\n",
    "\n",
    "def place_gaussian(arr, kernel, pos):\n",
    "    x, y, z = pos\n",
    "    kx, ky, kz = kernel.shape\n",
    "    # 计算高斯核在数组中的位置\n",
    "    x1, x2 = max(0, x-kx//2), min(arr.shape[0], x+kx//2+1)\n",
    "    y1, y2 = max(0, y-ky//2), min(arr.shape[1], y+ky//2+1)\n",
    "    z1, z2 = max(0, z-kz//2), min(arr.shape[2], z+kz//2+1)\n",
    "    # 计算高斯核在自身中的位置\n",
    "    kx1, kx2 = max(0, kx//2-x), min(kx, kx//2-x+arr.shape[0])\n",
    "    ky1, ky2 = max(0, ky//2-y), min(ky, ky//2-y+arr.shape[1])\n",
    "    kz1, kz2 = max(0, kz//2-z), min(kz, kz//2-z+arr.shape[2])\n",
    "    # 将高斯核放置在指定位置\n",
    "    arr[x1:x2,y1:y2,z1:z2] = np.maximum(arr[x1:x2,y1:y2,z1:z2], kernel[kx1:kx2,ky1:ky2,kz1:kz2])\n",
    "\n",
    "    return arr\n",
    "\n",
    "\n",
    "def create_mask(coordinates, shape, reduce=4, save=False, name=''):\n",
    "    \n",
    "    arr = np.zeros(tuple(np.array(shape) // reduce)) \n",
    "    for coord in coordinates:\n",
    "        x, y, z = coord[0: 3]\n",
    "        x = x / reduce\n",
    "        y = y / reduce\n",
    "        z = z / reduce \n",
    "        arr[int(x)][int(y)][int(z)] = 1\n",
    "    if save:\n",
    "        np.save('/public_bme/data/xiongjl/det//npy_data//{}_mask.npy'.format(name), arr)\n",
    "    \n",
    "    return arr\n",
    "\n",
    "\n",
    "def create_whd(coordinates, shape, reduce=4, save=False):\n",
    "    \n",
    "    arr = np.zeros(tuple(np.insert(np.array(shape) // reduce, 0, 3)))\n",
    "    for i in range(len(coordinates)):\n",
    "        x, y, z, w, h, d = coordinates[i]\n",
    "        x = x / reduce\n",
    "        y = y / reduce\n",
    "        z = z / reduce \n",
    "        arr[0][int(x)][int(y)][int(z)] = w\n",
    "        arr[1][int(x)][int(y)][int(z)] = h\n",
    "        arr[2][int(x)][int(y)][int(z)] = d\n",
    "    if save:\n",
    "        np.save('array.npy', arr)\n",
    "    \n",
    "    return arr\n",
    "\n",
    "\n",
    "def create_offset(coordinates, shape, reduce=4, save=False):\n",
    "    arr = np.zeros(tuple(np.insert(np.array(shape) // reduce, 0, 3)))\n",
    "    for coord in coordinates:\n",
    "        x, y, z = coord[0:3]\n",
    "        x = x / reduce\n",
    "        y = y / reduce\n",
    "        z = z / reduce \n",
    "        arr[0][int(x)][int(y)][int(z)] = x - int(x)\n",
    "        arr[1][int(x)][int(y)][int(z)] = y - int(y)\n",
    "        arr[2][int(x)][int(y)][int(z)] = z - int(z)\n",
    "    if save:\n",
    "        np.save('array.npy', arr)\n",
    "    return arr\n",
    "\n",
    "\n",
    "\n",
    "def create_hmap_v5(coordinates, shape):\n",
    "    arr = np.zeros(shape)\n",
    "    for coords in coordinates:\n",
    "        coord = [int(x) for x in coords[0:3]]\n",
    "        whd = [int(x) for x in coords[3:6]]\n",
    "        kernel = create_gaussian_kernel_v5(whd)\n",
    "        arr = place_gaussian(arr, kernel, coord)\n",
    "\n",
    "    return arr\n",
    "\n",
    "\n",
    "def compute_new_shape(old_shape, old_spacing, new_spacing) -> np.ndarray:\n",
    "    assert len(old_spacing) == len(old_shape)\n",
    "    assert len(old_shape) == len(new_spacing)\n",
    "    new_shape = np.array([int(round(i / j * k)) for i, j, k in zip(old_spacing, new_spacing, old_shape)])\n",
    "    return new_shape\n",
    "\n",
    "\n",
    "def generate_label(data_root_path, part, name, number):\n",
    "\n",
    "    img_path = data_root_path.joinpath(part).joinpath(name)\n",
    "    file_name = img_path.iterdir() # 迭代器不能够去进行索引\n",
    "    # print(f'img_path is {img_path}')\n",
    "    file_name = list(file_name)\n",
    "    if len(file_name) == 0:\n",
    "        print(f'the part : {part}, the name : {name} , have no data!!!!!!!!')\n",
    "    else:\n",
    "        img = tio.ScalarImage(os.path.join(img_path, file_name[0]))\n",
    "        source = os.path.join(img_path, file_name[0])\n",
    "        destination = f'D:\\\\Work_file\\\\uii_lymph_nodes_data\\\\DATASET\\\\imagesTr\\\\lymph_{number}_0000.nii.gz'\n",
    "        shutil.copy(source, destination)\n",
    "        # img.save(f'D:\\\\Work_file\\\\uii_lymph_nodes_data\\\\DATASET\\\\imagesTr\\\\lymph_{number}_0000.nii.gz')\n",
    "        # * 读取csv文件中的世界坐标\n",
    "        worldcoord = pd.read_csv(f'{data_root_path}/lymph_csv_refine/CTA_thin_std_{part}_lymph_refine.csv')\n",
    "        # csv_filename = f'{data_root_path}/lymph_csv_refine/{part}_npyrefine.csv'\n",
    "        csv_filename = f'/public_bme/data/xiongjl/lymph_det/csv_files/{part}_npyrefine.csv'\n",
    "        raw = worldcoord[worldcoord['image_path'].str.contains(name)]\n",
    "        coords = []\n",
    "        for i in range(len(raw)):\n",
    "            x = raw.iloc[i, 2]\n",
    "            y = raw.iloc[i, 3]\n",
    "            z = raw.iloc[i, 4]\n",
    "            width = raw.iloc[i, 5]\n",
    "            height = raw.iloc[i, 6]\n",
    "            depth = raw.iloc[i, 7]\n",
    "            coords.append([x, y, z, width, height, depth]) # 这个是世界坐标系\n",
    "        # print(f'the world coords is {coords}')\n",
    "\n",
    "        # * 把世界坐标系转化为图像坐标系\n",
    "        origin = img.origin\n",
    "        old_spacing = img.spacing\n",
    "        spacing = [0.78125, 0.78125, 1.0]\n",
    "        # spacing = [0.7889999747276306,\n",
    "        #         0.800000011920929,\n",
    "        #         0.7889999747276306,\n",
    "                \n",
    "        #     ]\n",
    "        old_shape = img.shape[1:]\n",
    "        shape = compute_new_shape(old_shape, old_spacing, spacing)\n",
    "        # print(f'the origin is {origin}')\n",
    "        img_coords = []\n",
    "        for coord in coords:\n",
    "            img_coord = (np.array(coord[0:3]) - np.array(origin) * np.array([-1., -1., 1.]) ) / np.array(spacing) # img.spacing\n",
    "            coord[3: 6] = coord[3: 6] / np.array(spacing)\n",
    "            img_coords.append([img_coord[0], img_coord[1], img_coord[2], coord[3], coord[4], coord[5]])   #! xyzwhd\n",
    "\n",
    "        for c in img_coord:\n",
    "            data = {'number': number,\n",
    "                    'name':name,\n",
    "                    'bbox':c,\n",
    "                    'origin':origin,\n",
    "                    'old_shape':old_shape,\n",
    "                    'new_shape':shape,\n",
    "                    'old_spacing':old_spacing}\n",
    "            df = pd.DataFrame(data)\n",
    "            # df.to_csv('your_file.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # # * 开始生成并且保存这个hmap\n",
    "        # hmap = create_hmap_v5(img_coords, shape)\n",
    "        # hmap = np.array(torch.tensor(hmap).unsqueeze(0))\n",
    "\n",
    "\n",
    "        # mask = create_mask(img_coords, shape, reduce=1) # 0.0s no save is so fast\n",
    "        # mask = np.array(torch.tensor(mask).unsqueeze(0))\n",
    "        # whd = create_whd(img_coords, shape, reduce=1)\n",
    "        # offset = create_offset(img_coords, shape, reduce=1)\n",
    "        # hmap = np.swapaxes(hmap, 1, 3)\n",
    "        # mask = np.swapaxes(mask, 1, 3)\n",
    "        # whd = np.swapaxes(whd, 1, 3)\n",
    "        # offset = np.swapaxes(offset, 1, 3)\n",
    "        # np.savez_compressed(f'D:\\\\Work_file\\\\uii_lymph_nodes_data\\\\DATASET\\\\npz_file\\\\lymph_{number}.npz', hmap=hmap, mask=mask, whd=whd, offset=offset)\n",
    "\n",
    "        # hmap = np.where(hmap >= 0.5, 1, 0)\n",
    "        # hmap_nii = tio.ScalarImage(tensor=torch.tensor(hmap), affine=img.affine)\n",
    "        # hmap_nii.save(f'D:\\\\Work_file\\\\uii_lymph_nodes_data\\\\DATASET\\\\test_labelTr\\\\lymph_{number}_hmap.nii.gz')\n",
    "        # whd_nii = tio.ScalarImage(tensor=torch.tensor(whd), affine=img.affine)\n",
    "        # whd_nii.save(f'D:\\\\Work_file\\\\uii_lymph_nodes_data\\\\DATASET\\\\test_labelTr\\\\lymph_{number}_whd.nii.gz')\n",
    "        # offset_nii = tio.ScalarImage(tensor=torch.tensor(offset), affine=img.affine)\n",
    "        # offset_nii.save(f'D:\\\\Work_file\\\\uii_lymph_nodes_data\\\\DATASET\\\\test_labelTr\\\\lymph_{number}_offset.nii.gz')\n",
    "        # mask_nii = tio.ScalarImage(tensor=torch.tensor(mask), affine=img.affine)\n",
    "        # mask_nii.save(f'D:\\\\Work_file\\\\uii_lymph_nodes_data\\\\DATASET\\\\test_labelTr\\\\lymph_{number}_mask.nii.gz')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    data_root_path = Path('D:\\\\Work_file\\\\uii_lymph_nodes_data')\n",
    "    parts = ['testing']\n",
    "\n",
    "\n",
    "    for part in parts:\n",
    "        names_list = []\n",
    "        with open(f'D:\\\\Work_file\\\\uii_lymph_nodes_data\\\\lymph_csv_refine\\\\{part}_names.csv') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for row in reader:\n",
    "                names_list.append(row[0])\n",
    "\n",
    "        # print(names_list)\n",
    "        # print(len(names_list))\n",
    "\n",
    "        work = {}\n",
    "        for i, name in tqdm(enumerate(names_list)):\n",
    "            i = i + 970\n",
    "            number = str(i)\n",
    "            if len(number) == 1:\n",
    "                number = f'00{number}'\n",
    "            elif len(number) == 2:\n",
    "                number = f'0{number}'\n",
    "            elif len(number) == 3:\n",
    "                pass\n",
    "            # else:\n",
    "                # print(f'the number given wrong, now is {number}')\n",
    "            work[number] = name\n",
    "        print(f'work: {work}')\n",
    "\n",
    "        for i in range(970, 1070):\n",
    "            number = str(i)\n",
    "            if len(number) == 1:\n",
    "                number = f'00{number}'\n",
    "            elif len(number) == 2:\n",
    "                number = f'0{number}'\n",
    "            name = work[number]\n",
    "            print(f'name:{name}-->number:{number}')\n",
    "            generate_label(data_root_path, part, name, number)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 100078.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work: {'970': '202004290225', '971': '202004100155', '972': '02200413220059', '973': '02200519222054', '974': '02191202220027', '975': '202004100173', '976': '202004070130', '977': '02191206218034', '978': '02200429178028', '979': '02200401220077', '980': '202004220181', '981': '202004300203', '982': '02191206218087', '983': '202005200015', '984': '02200401220069', '985': '02190816218011', '986': '02200422178032', '987': '02200407224040', '988': '02200403224024', '989': '02200420178052', '990': '202004020032', '991': '02200407220058', '992': '02200409216006', '993': '02191125224041', '994': '01190830220138', '995': '02200403220053', '996': '202004010197', '997': '02200409220058', '998': '202005200226', '999': '02200403216119', '1000': '202004070025', '1001': '202005200013', '1002': '02200403216100', '1003': '02200415216060', '1004': '02200407216029', '1005': '02200519218085', '1006': '202004010038', '1007': '202004280215', '1008': '02200403216109', '1009': '02200409224077', '1010': '02200520218093', '1011': '202004150039', '1012': '02200409178022', '1013': '202004090056', '1014': '02200519178064', '1015': '02200418220062', '1016': '202004070002', '1017': '02200519220132', '1018': '02200413178069', '1019': '202004020116', '1020': '02200416220174', '1021': '02200411218081', '1022': '02200403216037', '1023': '202004070056', '1024': '02200409216020', '1025': '202005190245', '1026': '202004140192', '1027': '02200407224131', '1028': '02191126220101', '1029': '02191127218143', '1030': '02200418220008', '1031': '202004080133', '1032': '02200407220105', '1033': '02200409220079', '1034': '02200415220041', '1035': '02200409216043', '1036': '02190830216107', '1037': '02200403220020', '1038': '202004140075', '1039': '202004100089', '1040': '02200403216143', '1041': '202004100182', '1042': '202004290069', '1043': '02200427216061', '1044': '02200409224006', '1045': '202004030079', '1046': '02190831216041', '1047': '02200407216110', '1048': '02200417178066', '1049': '02200407220020', '1050': '202004070018', '1051': '02200420178068', '1052': '02200520218138', '1053': '02200427220108', '1054': '02191206216104', '1055': '02200417224093', '1056': '02200401220107', '1057': '02200420220079', '1058': '02200520220001', '1059': '02200403216041', '1060': '02200520220123', '1061': '02200403220112', '1062': '02200429216007', '1063': '02200414178019', '1064': '02200414220057', '1065': '02190831220068', '1066': '02191203218005', '1067': '02191206220135', '1068': '02200427220085', '1069': '02200519216101'}\n",
      "name:202004290225-->number:970\n",
      "name:202004100155-->number:971\n",
      "name:02200413220059-->number:972\n",
      "name:02200519222054-->number:973\n",
      "name:02191202220027-->number:974\n",
      "name:202004100173-->number:975\n",
      "name:202004070130-->number:976\n",
      "name:02191206218034-->number:977\n",
      "name:02200429178028-->number:978\n",
      "name:02200401220077-->number:979\n",
      "name:202004220181-->number:980\n",
      "name:202004300203-->number:981\n",
      "name:02191206218087-->number:982\n",
      "name:202005200015-->number:983\n",
      "name:02200401220069-->number:984\n",
      "name:02190816218011-->number:985\n",
      "name:02200422178032-->number:986\n",
      "name:02200407224040-->number:987\n",
      "name:02200403224024-->number:988\n",
      "name:02200420178052-->number:989\n",
      "name:202004020032-->number:990\n",
      "name:02200407220058-->number:991\n",
      "name:02200409216006-->number:992\n",
      "name:02191125224041-->number:993\n",
      "name:01190830220138-->number:994\n",
      "name:02200403220053-->number:995\n",
      "name:202004010197-->number:996\n",
      "name:02200409220058-->number:997\n",
      "name:202005200226-->number:998\n",
      "name:02200403216119-->number:999\n",
      "name:202004070025-->number:1000\n",
      "name:202005200013-->number:1001\n",
      "name:02200403216100-->number:1002\n",
      "name:02200415216060-->number:1003\n",
      "name:02200407216029-->number:1004\n",
      "name:02200519218085-->number:1005\n",
      "name:202004010038-->number:1006\n",
      "name:202004280215-->number:1007\n",
      "name:02200403216109-->number:1008\n",
      "name:02200409224077-->number:1009\n",
      "name:02200520218093-->number:1010\n",
      "name:202004150039-->number:1011\n",
      "name:02200409178022-->number:1012\n",
      "name:202004090056-->number:1013\n",
      "name:02200519178064-->number:1014\n",
      "name:02200418220062-->number:1015\n",
      "name:202004070002-->number:1016\n",
      "name:02200519220132-->number:1017\n",
      "name:02200413178069-->number:1018\n",
      "name:202004020116-->number:1019\n",
      "name:02200416220174-->number:1020\n",
      "name:02200411218081-->number:1021\n",
      "name:02200403216037-->number:1022\n",
      "name:202004070056-->number:1023\n",
      "name:02200409216020-->number:1024\n",
      "name:202005190245-->number:1025\n",
      "name:202004140192-->number:1026\n",
      "name:02200407224131-->number:1027\n",
      "name:02191126220101-->number:1028\n",
      "name:02191127218143-->number:1029\n",
      "name:02200418220008-->number:1030\n",
      "name:202004080133-->number:1031\n",
      "name:02200407220105-->number:1032\n",
      "name:02200409220079-->number:1033\n",
      "name:02200415220041-->number:1034\n",
      "name:02200409216043-->number:1035\n",
      "name:02190830216107-->number:1036\n",
      "name:02200403220020-->number:1037\n",
      "name:202004140075-->number:1038\n",
      "name:202004100089-->number:1039\n",
      "name:02200403216143-->number:1040\n",
      "name:202004100182-->number:1041\n",
      "name:202004290069-->number:1042\n",
      "name:02200427216061-->number:1043\n",
      "name:02200409224006-->number:1044\n",
      "name:202004030079-->number:1045\n",
      "name:02190831216041-->number:1046\n",
      "name:02200407216110-->number:1047\n",
      "name:02200417178066-->number:1048\n",
      "name:02200407220020-->number:1049\n",
      "name:202004070018-->number:1050\n",
      "name:02200420178068-->number:1051\n",
      "name:02200520218138-->number:1052\n",
      "name:02200427220108-->number:1053\n",
      "name:02191206216104-->number:1054\n",
      "name:02200417224093-->number:1055\n",
      "name:02200401220107-->number:1056\n",
      "name:02200420220079-->number:1057\n",
      "name:02200520220001-->number:1058\n",
      "name:02200403216041-->number:1059\n",
      "name:02200520220123-->number:1060\n",
      "name:02200403220112-->number:1061\n",
      "name:02200429216007-->number:1062\n",
      "name:02200414178019-->number:1063\n",
      "name:02200414220057-->number:1064\n",
      "name:02190831220068-->number:1065\n",
      "name:02191203218005-->number:1066\n",
      "name:02191206220135-->number:1067\n",
      "name:02200427220085-->number:1068\n",
      "name:02200519216101-->number:1069\n",
      "数据已写入 nnunet_data_output_testing.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torchio as tio\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.ndimage import zoom\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from scipy.ndimage import binary_dilation\n",
    "from pathlib import Path\n",
    "import random\n",
    "import shutil\n",
    "#* trans the data to the test\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def compute_new_shape(old_shape, old_spacing, new_spacing) -> np.ndarray:\n",
    "    assert len(old_spacing) == len(old_shape)\n",
    "    assert len(old_shape) == len(new_spacing)\n",
    "    new_shape = np.array([int(round(i / j * k)) for i, j, k in zip(old_spacing, new_spacing, old_shape)])\n",
    "    return new_shape\n",
    "\n",
    "\n",
    "def generate_label(data_root_path, part, name, number, writer):\n",
    "\n",
    "    img_path = data_root_path.joinpath(part).joinpath(name)\n",
    "    file_name = img_path.iterdir() # 迭代器不能够去进行索引\n",
    "    # print(f'img_path is {img_path}')\n",
    "    file_name = list(file_name)\n",
    "    if len(file_name) == 0:\n",
    "        print(f'the part : {part}, the name : {name} , have no data!!!!!!!!')\n",
    "    else:\n",
    "        img = tio.ScalarImage(os.path.join(img_path, file_name[0]))\n",
    "        # source = os.path.join(img_path, file_name[0])\n",
    "        # destination = f'D:\\\\Work_file\\\\uii_lymph_nodes_data\\\\DATASET\\\\imagesTr\\\\lymph_{number}_0000.nii.gz'\n",
    "        # shutil.copy(source, destination)\n",
    "        # img.save(f'D:\\\\Work_file\\\\uii_lymph_nodes_data\\\\DATASET\\\\imagesTr\\\\lymph_{number}_0000.nii.gz')\n",
    "        # * 读取csv文件中的世界坐标\n",
    "        worldcoord = pd.read_csv(f'{data_root_path}/lymph_csv_refine/CTA_thin_std_{part}_lymph_refine.csv')\n",
    "        # csv_filename = f'{data_root_path}/lymph_csv_refine/{part}_npyrefine.csv'\n",
    "        csv_filename = f'/public_bme/data/xiongjl/lymph_det/csv_files/{part}_npyrefine.csv'\n",
    "        raw = worldcoord[worldcoord['image_path'].str.contains(name)]\n",
    "        coords = []\n",
    "        for i in range(len(raw)):\n",
    "            x = raw.iloc[i, 2]\n",
    "            y = raw.iloc[i, 3]\n",
    "            z = raw.iloc[i, 4]\n",
    "            width = raw.iloc[i, 5]\n",
    "            height = raw.iloc[i, 6]\n",
    "            depth = raw.iloc[i, 7]\n",
    "            coords.append([x, y, z, width, height, depth]) # 这个是世界坐标系\n",
    "        # print(f'the world coords is {coords}')\n",
    "\n",
    "        # * 把世界坐标系转化为图像坐标系\n",
    "        origin = img.origin\n",
    "        old_spacing = img.spacing\n",
    "        spacing = [0.78125, 0.78125, 1.0]\n",
    "\n",
    "        old_shape = img.shape[1:]\n",
    "        shape = compute_new_shape(old_shape, old_spacing, spacing)\n",
    "        # print(f'the new shape is {shape}')\n",
    "        img_coords = []\n",
    "        for coord in coords:\n",
    "            img_coord = (np.array(coord[0:3]) - np.array(origin) * np.array([-1., -1., 1.]) ) / np.array(spacing) # img.spacing\n",
    "            coord[3: 6] = coord[3: 6] / np.array(spacing)\n",
    "            img_coords.append([img_coord[0], img_coord[1], img_coord[2], coord[3], coord[4], coord[5]])   #! xyzwhd\n",
    "\n",
    "        for co in img_coords:\n",
    "            data_csv = [number, name, co, origin, old_shape, shape, old_spacing]\n",
    "            writer.writerow(data_csv)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    data_root_path = Path('D:\\\\Work_file\\\\uii_lymph_nodes_data')\n",
    "    parts = ['training']\n",
    "    part = 'testing'\n",
    "    # 指定要写入的CSV文件名\n",
    "    filename = \"nnunet_data_output_testing.csv\"\n",
    "    # for part in parts:\n",
    "    names_list = []\n",
    "    with open(f'D:\\\\Work_file\\\\uii_lymph_nodes_data\\\\lymph_csv_refine\\\\{part}_names.csv') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            names_list.append(row[0])\n",
    "\n",
    "        # print(names_list)\n",
    "        # print(len(names_list))\n",
    "\n",
    "    work = {}\n",
    "    for i, name in tqdm(enumerate(names_list)):\n",
    "        i = i+970\n",
    "        number = str(i)\n",
    "        if len(number) == 1:\n",
    "            number = f'00{number}'\n",
    "        elif len(number) == 2:\n",
    "            number = f'0{number}'\n",
    "        elif len(number) == 3:\n",
    "            pass\n",
    "        # else:\n",
    "            # print(f'the number given wrong, now is {number}')\n",
    "        work[number] = name\n",
    "    print(f'work: {work}')\n",
    "\n",
    "\n",
    "    with open(filename, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # writer.writerow([\"number\", \"name\", \"bbox\", \"origin\", \"old_shape\", \"new_shape\", \"old_spacing\"])\n",
    "\n",
    "        for i in range(970, 1070):\n",
    "            number = str(i)\n",
    "            if len(number) == 1:\n",
    "                number = f'00{number}'\n",
    "            elif len(number) == 2:\n",
    "                number = f'0{number}'\n",
    "            name = work[number]\n",
    "            print(f'name:{name}-->number:{number}')\n",
    "\n",
    "            generate_label(data_root_path, part, name, number, writer)\n",
    "\n",
    "        print(f\"数据已写入 {filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "det",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
